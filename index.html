<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Point Cloud to Mesh by shinmyung0</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Point Cloud to Mesh</h1>
      <h2 class="project-tagline">Point cloud data to mesh with possible down sampling.</h2>
      <a href="https://github.com/shinmyung0/pcltomesh" class="btn">View on GitHub</a>
      <a href="https://github.com/shinmyung0/pcltomesh/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/shinmyung0/pcltomesh/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="project-proposal" class="anchor" href="#project-proposal" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project Proposal</h1>

<h3>
<a id="1-team" class="anchor" href="#1-team" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Team</h3>

<p>Shin Yoon</p>

<h3>
<a id="2-background" class="anchor" href="#2-background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Background</h3>

<p>The idea is simple. A point cloud is just a bunch of unordered vertices. What we want to do is to group these vertices into triangles to form a mesh. Why is this useful? This is extremely useful in fields like 3D reconstruction, where you have large amounts of point cloud data from RGBD cameras or lidar, and you want to recreate a mesh representing the object that you scanned. This is how meshes of objects scanned for 3D printers are generated. This is also useful in SLAM(Simultaneous Localization and Mapping) for robots trying to create a 3D reconstruction of their surroundings. </p>

<p>This is the reason why this project is interesting for me personally. I'm currently involved in research regarding super-resolution SLAM on UAVs. The idea is to use localized RGBD point cloud data and then generate a mesh of the surroundings. Then using a high resolution camera map textures to this mesh of a view that the UAV is "seeing." The first part of this research project is something that I've been interested in, so I wanted to try to implement the basics within the scope of this project. </p>

<p>To start off I plan to try to implement <a href="http://www.research.ibm.com/vistechnology/pdf/bpa_tvcg.pdf">this paper</a> using the mesh data provided <a href="http://graphics.stanford.edu/data/3Dscanrep/">here</a>. Afterwards, once I get it working on standard data sets, I want to try to implement a form of down sampling as well to try and simplify the point cloud before running a meshing algorithm on it. This part is important because in our own research, the point clouds that are generated are extremely dense, but a lot of the data can be reduced and still generate a nice mesh. But the main goal for this project is to actual implement the mesh generation algorithm.</p>

<h3>
<a id="3-resources" class="anchor" href="#3-resources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Resources</h3>

<p>Language + Platform : C++/ OpenGL 
Libraries : Point Cloud Library (File parsing, Data manipulation), Eigen (Vector, Matrix), CS184 HW2 MeshEdit</p>

<p>I am planning on using our hw2 code base to use as the starting point. I think the visualizer was useful and using that as the starting point will help me debug and implement faster. </p>

<h3>
<a id="4-goals-and-deliverables" class="anchor" href="#4-goals-and-deliverables" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4. Goals and Deliverables</h3>

<p>My main deliverable will be to demo the mesh generation algorithm using vertices from the <a href="stanford_3d">Stanford 3D Scanning Repository</a>. I plan to implement the <a href="bpa">Ball Pivot Algorithm</a> to show how vertices can be grouped together to form a mesh.</p>

<p><img src="http://cs184.eecs.berkeley.edu/cs184_sp16_content/article_images/21_2.jpg" alt="Point Cloud to Mesh"></p>

<p>In terms of aspirational goals, I want to also implement the <a href="poisson">Poisson Surface Reconstruction Algorithm</a> and do a comparison of the two methods on a model. Then if that goes well, I want to try to run my algorithm on actual point cloud data from the SLAM program that we are running in my research project. This would hopefully generate a rough mesh of whatever the RGBD camera is seeing localized. This would involve writing a <a href="ros_pubsub">ROS Publisher/Subscriber</a> wrapper around my mesh generator, and streaming the data from the SLAM program and back out into <a href="rviz">Rviz</a>. </p>

<h3>
<a id="5-intended-schedule" class="anchor" href="#5-intended-schedule" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>5. Intended Schedule</h3>

<ul>
<li>April 24th - Finish the Ball Pivot Algorithm + Downsampling + augment HW2 code base to incorporate it</li>
<li>May 1st - try to implement Poisson Surface Reconstruction and compare with Ball Pivot Algorithm</li>
<li>May 3rd - Possibly look into ROS integration and writing a publisher/subscriber, prepare demo.</li>
</ul>

<h1>
<a id="final-report-as-of-may-4th" class="anchor" href="#final-report-as-of-may-4th" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Final Report (As of May 4th)</h1>

<h3>
<a id="1-summary" class="anchor" href="#1-summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Summary</h3>

<p>This is the final report write up for CS184 Spring 2016. I finished my main goal of implementing the Ball Pivoting Algorithm to generate a mesh from point cloud data, although I did not quite reach my aspirational goals. In the process of implementing this project, I learned a lot about the trade offs involved in generating a mesh from point cloud data. I plan to continue to develop on this project even after this semester, and hopefully reach my aspirational goals some time in the future, mainly testing out Poisson Surface Reconstruction as well as integration with UAV point cloud data. There were some divergences from my original proposal, mainly involving the data sets being used. I originally planned to use the Stanford 3D Scanning Repository, but when I opened the .ply files, I found that there were already polygon relationships defined inside. I wanted to test on more realistic point cloud data, so instead I found <a href="pcd_files">a repository of .pcd files</a> containing only vertices. This format was specifically designed to contain point cloud data, and also the format was well supported within the Point Cloud Library(PCL).</p>

<h3>
<a id="2-abstract" class="anchor" href="#2-abstract" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Abstract</h3>

<p>The main focus of this project was to implement the Ball Pivot algorithm on point cloud data. The overall algorithm followed the previously mentioned paper closely. The basic idea of the algorithm is simple : there is an "advancing front" of active edges from which a ball of radius, p, is rotated around until the ball "hits" a vertex in which case the vertex is joined together with the current mesh. Newly created edges are joined to the mesh, and older edges are removed from the front. To start the algorithm, there is a seed triangle the defines the initial front from which the ball starts to pivot from.</p>

<p><img src="http://gdurl.com/kBDc" alt="Advancing Front"></p>

<p><em>Advancing Front</em></p>

<h3>
<a id="31-parsing-and-structuring-the-point-cloud-data" class="anchor" href="#31-parsing-and-structuring-the-point-cloud-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3.1 Parsing and Structuring the Point Cloud Data</h3>

<p>To parse and structure the data I used PCL. PCL offers a PCDReader to easily parse .pcd files into their PointCloud data structure. From this I used PCL's KDTree to organize the parsed vertices. I did this because the Ball Pivot Algorithm(BPA) involves pivoting a ball in space and determining whether certain vertices are hit, thus requiring a lot of spatial queries. A KDTree is very efficient in doing nearest neighbor spatial queries, also PCL had excellent support for spatially partitioning points as well as searching inside a KDTree.</p>

<h3>
<a id="32-calculating-the-normal" class="anchor" href="#32-calculating-the-normal" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3.2 Calculating the normal</h3>

<p>Calculating normal vectors for unstructured point cloud data comes down to estimating the normal by fitting a least-squares plane around k-neighboring points. For determining the sign of the normal, we can assume to orient everything towards the viewpoint. PCL already provides methods for doing this using outlined <a href="normals">here</a>. </p>

<p><img src="http://gdurl.com/koAO" alt="kneighbors"></p>

<h3>
<a id="33-selecting-a-seed-triangle" class="anchor" href="#33-selecting-a-seed-triangle" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3.3 Selecting a Seed Triangle</h3>

<p>An initial front of edges is created by selecting a seed triangle. I selected a seed triangle by getting any random point not yet used by the algorithm, then searching with the nearest neighboring points until I found 2 other points that formed a triangle that satisfied two conditions: the triangle is consistent with the surface normals and a p-radius ball contains all three points without having any other points inside. The initial front is the edges of this triangle.</p>

<p><img src="http://gdurl.com/2dPf" alt="seeds"></p>

<h3>
<a id="34-ball-pivoting" class="anchor" href="#34-ball-pivoting" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3.4 Ball Pivoting</h3>

<p>Anytime we pivot the ball, we need to look within a radius p for points the the ball would hit as it pivoted long an edge. We need to query points within a 2*p distance from the points that we are querying from. Starting from an active edge, we first calculate the midpoint of the edge,  For every point within this neighborhood we compute the center of a sphere touch v0, v1 (points from the original edge), and vn(new point we are testing), if such a ball exists. The center of this sphere, cx, intersects with the circular path that our ball draws, in two points. We choose the first of these points. For all points taken into consideration, we must also consider whether or not the generated face's normal is consistent with the face that the pivot edge belongs to.</p>

<p><img src="http://patentimages.storage.googleapis.com/US6968299B1/US06968299-20051122-D00000.png" alt="3D Representation"></p>

<h3>
<a id="4-results-and-technical-difficulties" class="anchor" href="#4-results-and-technical-difficulties" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4. Results and Technical Difficulties</h3>

<p>Mesh generation of a single side of the Stanford Bunny. Tried with various radius sizes, r=0.01, 0.02 displayed here. You can see as the radius of the ball increases more points are missed and you get a lower resolution mesh. The mesh generation is far from perfect, but the original point cloud itself was a little sparse.</p>

<p><img src="http://gdurl.com/Hmrc" alt="original_mesh"></p>

<p><em>Original Mesh</em></p>

<p><img src="http://gdurl.com/64MI" alt="r001"></p>

<p><em>Radius 0.01 Generated Mesh</em></p>

<p><img src="http://gdurl.com/bdeQ" alt="r002"></p>

<p>I tried it with a bigger point cloud of a few thousand points. This point cloud has around ~3500 points. The mesh was generated with a radius of 5.0. You can see that the tail area of the point cloud cat has points spaced much farther apart, and this causes disconnected parts of the mesh. The tail is still partially rendered because the algorithm keeps generating the mesh from new seed triangles until they can no longer be generated. </p>

<p><img src="http://gdurl.com/J0dY" alt="cat_cloud"></p>

<p><em>Original Point Cloud ~3500 points</em></p>

<p><img src="http://gdurl.com/rlmk" alt="cat_mesh"></p>

<p><em>Radius 5.0 Generated Mesh</em></p>

<p>The hardest thing about this algorithm was deciding on the correct radius value for my pivoting ball. During simple testing I just looked at the vertex values and set some random scale for my ball radius, but some meshes have varied areas of dense point clouds versus sparse point clouds. The tail area of the cat mesh shows what happens when certain areas of the mesh have sparser clouds than others. The main body was generated well, but the tail had disconnected parts of the meshes. I was playing around with ways to dynamically set the radius based off of sampled subsets of the points as well as increasing the pivot ball's radius at an active edge and trying again when it couldn't find a point. But I couldn't get it to work successfully in time. This is one of the things that I plan to improve upon in the future.</p>

<p>Another thing I found difficult was that despite using PCL's normal estimator, a lot of my surface normals were not being approximated correctly. I'm not sure if this is because there is a bug in my code, or if I did not set a high enough k neighbor value for plane fitting, but I found that because of incorrect normals some mesh surfaces, especially around sharp curvatures such as feet, tails, or other circular meshes. You can see this effect in the cat mesh as well. Some parts of the surface have normals inconsistent with the rest of the mesh that cause the feet and face part of the cat to have mesh generation be cut off. This is also something I want to improve and fix in the future.</p>

<h3>
<a id="future-todo-list" class="anchor" href="#future-todo-list" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Future Todo List</h3>

<ul>
<li>Fix bugs in normal estimation</li>
<li>Dynamic ball radius</li>
<li>Implement some of the out of core algorithms talked about in the original paper to handle larger meshes</li>
<li>Try incorporating streaming point cloud data from a UAV using ROS</li>
</ul>

<h3>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>

<p>For this project, I heavily referenced the <a href="pcldocs">PCL documentation</a>. I also read more about the algorithm and used some images from <a href="bpapres">this</a> presentation I found on the web.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/shinmyung0/pcltomesh">Point Cloud to Mesh</a> is maintained by <a href="https://github.com/shinmyung0">shinmyung0</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
