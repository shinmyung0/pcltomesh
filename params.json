{
  "name": "Point Cloud to Mesh",
  "tagline": "Point cloud data to mesh with possible down sampling.",
  "body": "### Team\r\nShin Yoon. Just want to try doing this on my own.\r\n\r\n### Background\r\nThe idea is simple. A point cloud is just a bunch of unordered vertices. What we want to do is to group these vertices into triangles to form a mesh. Why is this useful? This is extremely useful in fields like 3D reconstruction, where you have large amounts of point cloud data from RGBD cameras or lidar, and you want to recreate a mesh representing the object that you scanned. This is how meshes of objects scanned for 3D printers are generated. This is also useful in SLAM(Simultaneous Localization and Mapping) for robots trying to create a 3D reconstruction of their surroundings. \r\n\r\nThis is the reason why this project is interesting for me personally. I'm currently involved in research regarding super-resolution SLAM on UAVs. The idea is to use localized RGBD point cloud data and then generate a mesh of the surroundings. Then using a high resolution camera map textures to this mesh of a view that the UAV is \"seeing.\" The first part of this research project is something that I've been interested in, so I wanted to try to implement the basics within the scope of this project. \r\n\r\nTo start off I plan to try to implement [this paper](http://www.research.ibm.com/vistechnology/pdf/bpa_tvcg.pdf) using the mesh data provided [here](http://graphics.stanford.edu/data/3Dscanrep/). Afterwards, once I get it working on standard data sets, I want to try to implement a form of down sampling as well to try and simplify the point cloud before running a meshing algorithm on it. This part is important because in our own research, the point clouds that are generated are extremely dense, but a lot of the data can be reduced and still generate a nice mesh. But the main goal for this project is to actual implement the mesh generation algorithm.\r\n\r\n### Resources\r\nLanguage + Platform : C++/ OpenGL \r\n\r\nI am planning on using our hw2 code base to use as the starting point. I think the visualizer was useful and using that as the starting point will help me debug and implement faster. \r\n\r\n### Goals and Deliverables\r\n\r\nMy main deliverable will be to demo the mesh generation algorithm using vertices from the [Stanford 3D Scanning Repository](http://graphics.stanford.edu/data/3Dscanrep/). I plan to implement the [ball pivot algorithm](http://www.research.ibm.com/vistechnology/pdf/bpa_tvcg.pdf) to show how vertices can be grouped together to form a mesh.\r\n\r\n![Point Cloud to Mesh](http://cs184.eecs.berkeley.edu/cs184_sp16_content/article_images/21_2.jpg)\r\n\r\nIn terms of aspirational goals, I want to also implement the [Poisson Surface Reconstruction Algorithm](http://research.microsoft.com/en-us/um/people/hoppe/poissonrecon.pdf) and do a comparison of the two methods on a model. Then if that goes well, I want to try to run my algorithm on actual point cloud data from the SLAM program that we are running in my research project. This would hopefully generate a rough mesh of whatever the RGBD camera is seeing localized. This would involve writing a [ROS Publisher/Subscriber] (http://wiki.ros.org/ROS/Tutorials/WritingPublisherSubscriber) wrapper around my mesh generator, and streaming the data from the SLAM program and back out into [Rviz](http://wiki.ros.org/rviz). \r\n\r\n### Schedule\r\n- April 24th - Finish the Ball Pivot Algorithm + Downsampling + augment HW2 code base to incorporate it\r\n- May 1st - try to implement Poisson Surface Reconstruction and compare with Ball Pivot Algorithm\r\n- May 3rd - Possibly look into ROS integration and writing a publisher/subscriber, prepare demo.\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}